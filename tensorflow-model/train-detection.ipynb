{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Presets\n",
    "\n",
    "1. Run setup notebook!\n",
    "2. Install object detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Object Detection API\n",
    "%cd models/research/\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "!cp object_detection/packages/tf2/setup.py .\n",
    "!python -m pip install --user ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and prepare dataset\n",
    "\n",
    "Download the 2017 [MSCOCO](https://cocodataset.org/#home) dataset. The download script focus only on person annotations and images since we want to train for person detection only.\n",
    "The script skips download if the .zip-file already exists. Unzip is skipped if the folder already exists. This file is supplied via the git repository and must be moved into the tensorflow models subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy custom download script to corresponding location to execute (depends on scripts of object_detection/dataset_tools/)\n",
    "# But keep copy in this dir for changes to git\n",
    "%cd ~/sparky-rooms/tensorflow-model/\n",
    "%cp download_and_preprocess_mscoco_person.sh models/research/object_detection/dataset_tools/\n",
    "# Go to root of tensorflow sources\n",
    "%cd models/research/\n",
    "# Run script to download and unzip to data/mscoco if not already existing\n",
    "!bash object_detection/dataset_tools/download_and_preprocess_mscoco_person.sh ~/dataset/mscoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training\n",
    "\n",
    "Reference configured pipeline from git repository and assign output directory of the training. The checkpoint of the resulting detection model is located here. This checkpoint is later used to test the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG=\"sparky-rooms/tensorflow-model/pipeline_nonfpn.config\"\n",
    "MODEL_OUT=\"sparky-rooms/tensorflow-model/sparky_nonfpn\"\n",
    "\n",
    "# Run training\n",
    "%cd ~\n",
    "!python sparky-rooms/tensorflow-model/models/research/object_detection/model_main_tf2.py \\\n",
    "--model_dir=$MODEL_OUT \\\n",
    "--num_train_steps=1000000 \\\n",
    "--sample_1_of_n_eval_examples=1 \\\n",
    "--pipeline_config_path=$CONFIG \\\n",
    "--alsologtostderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Test resulting checkpoint inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many results should be printed:\n",
    "SLICE=10\n",
    "# Checkpoint to load\n",
    "CHK='ckpt-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import visualization_utils as viz_util\n",
    "\n",
    "print(\"Load pipeline config and build a detection model...\")\n",
    "configs = config_util.get_configs_from_pipeline_file(\"/home/jupyter/\"+CONFIG)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "print(\"Restore checkpoint...\")\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(\"/home/jupyter/\"+MODEL_OUT, CHK)).expect_partial() # Can vary\n",
    "\n",
    "print(\"Load image for inference testing ...\")\n",
    "image = Image.open('/home/jupyter/data/test.jpg').resize((320,320))\n",
    "image_data = np.asarray(image)\n",
    "#image_data = image_data.astype(np.float32)\n",
    "\n",
    "# Feed as input\n",
    "input_data = np.asarray([image_data])\n",
    "input_data = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
    "\n",
    "print(\"Run inference...\")\n",
    "image, shapes = detection_model.preprocess(input_data)\n",
    "prediction_dict = detection_model.predict(input_data, shapes)\n",
    "detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "detection_scores = detections['detection_scores'][0][:SLICE].numpy()\n",
    "detection_classes = detections['detection_classes'][0][:SLICE].numpy().astype(np.int32)\n",
    "detection_boxes = detections['detection_boxes'][0][:SLICE].numpy()\n",
    "\n",
    "label_map = label_map_util.load_labelmap(\"/home/jupyter/sparky-rooms/tensorflow-model/label_map.pbtxt\")\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=10, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "print(\"Categories:\")\n",
    "print(categories)\n",
    "print(category_index)\n",
    "\n",
    "image_infered = viz_util.visualize_boxes_and_labels_on_image_array(\n",
    "    image_data.copy(),\n",
    "    np.squeeze(detection_boxes),\n",
    "    np.squeeze(detection_classes),\n",
    "    np.squeeze(detection_scores),\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    line_thickness=8)\n",
    "\n",
    "display(Image.fromarray(image_infered))\n",
    "\n",
    "# Results\n",
    "print('Shopwing first {} Results:'.format(SLICE))\n",
    "print(\"Detection Scores:\")\n",
    "print(detection_scores)\n",
    "print(\"Detection Classes:\")\n",
    "print(detection_classes)\n",
    "print(\"Detection Boxes:\")\n",
    "print(detection_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export model\n",
    "\n",
    "Export the model from checkpoint to .pb (freeze graph). Then convert the frozen graph into .tflite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS=\"sparky/results\"\n",
    "MODEL_NO_QUANT_TFLITE=\"sparky/results/model_no_quant.tflite\"\n",
    "MODEL_MICRO=\"sparky/results/model.cc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "# Export checkpoint to .pb\n",
    "!python sparky-rooms/tensorflow-model/models/research/object_detection/exporter_main_v2.py \\\n",
    "--pipeline_config_path=$CONFIG \\\n",
    "--trained_checkpoint_dir=$MODEL_OUT \\\n",
    "--output_directory=$RESULTS \\\n",
    "--input_type image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "%cd ~\n",
    "# Export checkpoint to .pb - file name \"export_tflite_graph_tf2\" is missleading. This generates a .pb file which can be converted into tflite.\n",
    "!python sparky-rooms/tensorflow-model/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
    "--pipeline_config_path=$CONFIG \\\n",
    "--trained_checkpoint_dir=$MODEL_OUT \\\n",
    "--output_directory=$RESULTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert .pb to .Lite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"/home/jupyter/\"+RESULTS+\"/saved_model\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops = True\n",
    "model_no_quant_tflite = converter.convert()\n",
    "\n",
    "open(\"/home/jupyter/\"+MODEL_NO_QUANT_TFLITE, \"wb\").write(model_no_quant_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test converted tflite model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/jupyter/\"+MODEL_NO_QUANT_TFLITE)\n",
    "#interpreter = tf.lite.Interpreter(model_path=\"/home/jupyter/ssd_mobilenet_v1_1_metadata_1.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape']\n",
    "output_shape = output_details[0]['shape']\n",
    "\n",
    "print(\"Expected input shape: \", input_shape)\n",
    "print(\"Expected output shape: \", output_shape)\n",
    "\n",
    "# Load test image\n",
    "print(\"Prepare image...\")\n",
    "image = Image.open('/home/jupyter/data/test3.jpg').resize((input_shape[1], input_shape[2]))\n",
    "image_data = np.asarray(image)\n",
    "#image_data = image_data.astype(np.float32)\n",
    "\n",
    "# Feed as input\n",
    "input_data = np.asarray([image_data])\n",
    "print(\"Set input...\")\n",
    "tensor_index = input_details[0]['index']\n",
    "#input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "#input_tensor[:, :] = input_data\n",
    "interpreter.set_tensor(tensor_index, input_data)\n",
    "\n",
    "print(\"Invoke inference...\")\n",
    "# Inference\n",
    "interpreter.invoke()\n",
    "print(\"Finished inference...\")\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "# Get output data\n",
    "detection_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "detection_classes = interpreter.get_tensor(output_details[1]['index']).astype(np.int32)\n",
    "detection_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "num_boxes = interpreter.get_tensor(output_details[3]['index'])\n",
    "\n",
    "label_map = label_map_util.load_labelmap(\"/home/jupyter/sparky-rooms/tensorflow-model/label_map.pbtxt\")\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=10, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "print(\"Categories:\")\n",
    "print(categories)\n",
    "print(category_index)\n",
    "\n",
    "image_infered = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "    image_data.astype(np.uint8).copy(),\n",
    "    np.squeeze(detection_boxes),\n",
    "    np.squeeze(detection_classes),\n",
    "    np.squeeze(detection_scores),\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    line_thickness=8)\n",
    "\n",
    "display(Image.fromarray(image_infered))\n",
    "\n",
    "print(\"Result:\")\n",
    "print(num_boxes)\n",
    "print(detection_boxes)\n",
    "print(detection_classes)\n",
    "print(detection_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create final output\n",
    "\n",
    "Convert to binary C array which can be loaded onto a arduino board if its size is small enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a C source file\n",
    "%cd ~\n",
    "!apt-get update && apt-get -qq install xxd\n",
    "!xxd -i {MODEL_NO_QUANT_TFLITE} > {MODEL_MICRO}\n",
    "REPLACE_TEXT = MODEL_NO_QUANT_TFLITE.replace('/', '_').replace('.', '_')\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_MICRO}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
